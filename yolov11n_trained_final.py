# -*- coding: utf-8 -*-
"""yolov11n_trained_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EGdSDNyj5MmGSbpIlHAbXsYjNcsbiEwe

# Training YOLO v11 Models in Google Colab

This notebook utilizes [Ultralytics](https://docs.ultralytics.com/) to train a YOLOv11n object detection model on a custom dataset.
By the end of this notebook, you will have a trained YOLO model ready to deploy on various platforms, including your PC, smartphone, or edge devices such as the SAMA7D65 Curiosity board.


### Working in Colab
Google Colab provides a cloud-based development environment with a full Linux OS, preconfigured Python ecosystem, and access to GPU acceleration. This makes it ideal for training deep learning models without requiring local compute resources. All training will be performed using PyTorch and Ultralytics, with support for exporting models to ONNX or TFLite formats for edge deployment.

### Notebook Navigation
Use the left-hand table of contents for quick access to different sections such as environment setup, dataset preparation, training, evaluation, and deployment/export workflows.

**Verify GPU Availability**

To ensure optimal training performance, confirm that your runtime is configured to use a GPU:

1.Navigate to Runtime ‚Üí Change runtime type in the Colab menu bar.

2.Under Hardware accelerator, select GPU.

3.Click Save to apply the changes.

Then, execute the following code cell to verify that an NVIDIA GPU is available and properly recognized by the environment.

A successful configuration will display the GPU model, driver version, and memory status. This step is essential for accelerated model training using PyTorch and Ultralytics.
"""

!nvidia-smi

"""# 1.&nbsp;Install Required Packages (Ultralytics)
In this step, we‚Äôll install the Ultralytics library into the current Colab environment. This library provides a high-level API for training and deploying YOLO models efficiently. It includes built-in support for training, validation, export, and deployment to a variety of platforms.

Run the following code block to install Ultralytics and its dependencies
"""

!pip install ultralytics

"""# 2.&nbsp;Upload Dataset and Prepare for Training

In this step, we will upload the custom image dataset and organize it for use with YOLO training workflows. This includes:

1.Upload the Dataset

2.Splitting the dataset into training and validation sets

3.Structuring the directories according to YOLO format


This setup ensures that the dataset is correctly formatted and ready for training with YOLOv11.

## 2.1 Upload images

First, we need to upload the dataset to Colab. Here are a few options for moving the `data_classes.zip` folder into this Colab instance.

**Option 1: Upload Dataset Directly to Colab Session**

For smaller datasets or quick experiments, you can upload your dataset (data.zip) directly to this Colab session:

1.Click the Files icon (üìÅ) on the left sidebar.

2.Click the Upload to session storage icon (üì§).

3.Select your data.zip file and upload it.

Once uploaded, the dataset will be temporarily available in the Colab filesystem for use during this session.

‚ö†Ô∏è Note: Files uploaded this way will be lost once the session ends. For larger or persistent datasets, consider using Google Drive (Option 2).

<p>
<br>
<img src="https://drive.google.com/uc?id=1USh1GjHK5V0uDDQDv9oSPDAfCvIciVcq" height="240">
</p>

**Option 2. Load Dataset from Google Drive**

If your dataset exceeds ~100‚ÄØMB or you want to avoid uploading it manually in each Colab session, you can store the dataset in your Google Drive and access it from Colab.

This block will:

1.Mount your Google Drive

2.Copy the dataset archive (.zip) into the Colab environment

3.Prepare it for further extraction and use

Before running the cell below to mount the google drive and copy the folder to this filesystem, make sure you've uploaded your dataset (e.g., data_classes.zip) to a folder in Google Drive and update the file path accordingly (e.g., MyDrive/Fruits_Vegetables_dataset/data_classes.zip).
"""

from google.colab import drive
drive.mount('/content/gdrive')

!cp /content/gdrive/MyDrive/Fruits_Vegetables_dataset/data_classes.zip /content

"""## 2.2 Splitting Dataset

At this point, whether you used Option 1 (Google Drive) or Option 2 (direct upload), you should now see your data.zip file in the file browser on the left (üìÅ icon).

Next, we‚Äôll unzip data.zip and create the required folder structure to organize the images and labels for training.

Run the following code block to extract the dataset and prepare it for the next step.
"""

# Unzip images to a custom data folder
!unzip -q /content/data_classes.zip -d /content/custom_data

"""**Prepare the Dataset Folder Structure**

Ultralytics requires a specific directory structure for training object detection models. The root directory is typically named data/, and it must contain the following structure:

train/images/ ‚Äì Image files used for training

train/labels/ ‚Äì Corresponding YOLO-format label files

validation/images/ ‚Äì Image files used for validation

validation/labels/ ‚Äì Corresponding YOLO-format label files

During training, the model uses all images in the train set to adjust its internal weights, while the validation set is used to evaluate model performance after each epoch.

To simplify this setup, we‚Äôll use a Python script that:

1.Automatically creates the required folder structure

2.Randomly splits your dataset (90% for training, 10% for validation)

3.Moves the image and label files accordingly

Run the following code block to copy and execute the script.
"""

!cp /content/gdrive/MyDrive/train_val_split.py /content

!python /content/train_val_split.py --datapath="/content/custom_data/data_classes/" --train_pct=0.9

"""# 3.&nbsp;Creating YOLO Configuration files

Before starting training, we need to create a configuration YAML file for Ultralytics. This file defines:

1.The paths to your training and validation datasets

2.The list of class names for your model

An example configuration file is available here for reference.

Run the next code block to automatically generate a data.yaml file tailored to your dataset.

‚ö†Ô∏èNote: Ensure your label map file classes.txt is located at custom_data/classes.txt.

If you used Label Studio or one of the provided datasets, this file should already be present.

If you created the dataset manually, you may need to create classes.txt yourself.
See this generated example file for formatting guidance.
"""

# Python function to automatically create data.yaml config file
# Reads the classes.txt file to extract the list of class names
# Builds a configuration dictionary with the appropriate dataset paths, number of classes, and class names
# Writes the configuration to a data.yaml file in the correct YAML format
import yaml
import os

def create_data_yaml(path_to_classes_txt, path_to_data_yaml):

  # Read class.txt to get class names
  if not os.path.exists(path_to_classes_txt):
    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')
    return
  with open(path_to_classes_txt, 'r') as f:
    classes = []
    for line in f.readlines():
      if len(line.strip()) == 0: continue
      classes.append(line.strip())
  number_of_classes = len(classes)

  # Create data dictionary
  data = {
      'path': '/content/data',
      'train': 'train/images',
      'val': 'validation/images',
      'nc': number_of_classes,
      'names': classes
  }

  # Write data to YAML file
  with open(path_to_data_yaml, 'w') as f:
    yaml.dump(data, f, sort_keys=False)
  print(f'Created config file at {path_to_data_yaml}')

  return

# Define path to classes.txt and run function
path_to_classes_txt = '/content/custom_data/data_classes/classes.txt'
path_to_data_yaml = '/content/data.yaml'

create_data_yaml(path_to_classes_txt, path_to_data_yaml)

print('\nFile contents:\n')
!cat /content/data.yaml

"""# 4.&nbsp;Training the Custom YOLOv11n Model

With the dataset structured correctly and the configuration file in place, you're now ready to begin training your YOLO model.

Before proceeding, review and customize the key training parameters‚Äîsuch as number of epochs, batch size, and image resolution‚Äîto suit your specific use case and hardware constraints.

## 4.1 Training Parameters
Before launching training, it's important to configure a few key parameters that directly impact model performance and training time.

**Epochs ‚Äî Number of Training Epochs**

An epoch is one full pass through the training dataset. The optimal number of epochs depends on dataset size and model complexity.

For small datasets (< 200 images): start with 60 epochs

For larger datasets (‚â• 200 images): start with 40 epochs
You can adjust this number later based on model performance and training duration.

**imgsz ‚Äî Input Image Resolution**

Input resolution significantly affects both inference speed and detection accuracy:

640√ó640 is the standard resolution for YOLO models

320√ó320 or lower may be preferred for faster inference or lower-resolution inputs

Higher resolutions can improve accuracy at the cost of increased computation and slower training.

## 4.2 Run Training!

Run the following code block to begin training.
"""

!yolo detect train data=/content/data.yaml model=yolo11n.pt epochs=40 imgsz=320 project="/content/gdrive/MyDrive/yolov11n_project/" name="train"

"""#How Training Works
Once training begins, the algorithm iteratively processes the images in the training and validation directories. At the end of each epoch, the model is evaluated on the validation set, and key metrics such as mAP (mean Average Precision), precision, and recall are computed and reported.

As training progresses, you should observe an overall increase in mAP, indicating improved model performance.

‚ö†Ô∏è Important: Do not interrupt training prematurely. At the conclusion of the final epoch, a post-processing step is executed to strip unnecessary components from the model and optimize it for deployment.

The best-performing model weights (based on validation mAP) are saved to:
content/runs/detect/train/weights/best.pt

Additional outputs, such as loss curves and metric plots, are saved in:
content/runs/detect/train/
This includes a results.png image that visualizes training metrics across epochs.

#5.&nbsp;Validate / Test Model

###Evaluate the Trained Model
Now that training is complete, we can evaluate the model‚Äôs performance on the validation dataset. The following code will:

Run inference using the trained model on the validation images.

Visualize predictions for the first 10 images.

This step is essential to verify that the model generalizes well to unseen data and to get a quick visual confirmation that the training process was successful.

Click the Play button on the cells below to begin evaluation.
"""

!yolo detect predict model=/content/gdrive/MyDrive/yolov11n_project/train/weights/best.pt source=data/validation/images save=True

import glob
from IPython.display import Image, display
for image_path in glob.glob(f'/content/runs/detect/predict/*.jpg')[:10]:
  display(Image(filename=image_path, height=400))
  print('\n')

"""Run validation on the trained model to evaluate its performance using mAP metrics across all classes."""

from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # load an official model
model = YOLO("/content/gdrive/MyDrive/yolov11n_project/train/weights/best.pt")  # load a custom model

# Validate the model
metrics = model.val()  # no arguments needed, dataset and settings remembered
metrics.box.map  # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps  # a list contains map50-95 of each category

"""#6.&nbsp;Deploy Model

Now that your custom model has been trained, it's ready for download and deployment in real-world applications. YOLO models are highly versatile and can run on PCs, embedded platforms, and mobile devices. Ultralytics provides tools to convert models to formats like tflite and onnx for deployment in diverse environments.

This section explains how to download your trained model and includes guidance for deploying it on edge devices such as the **SAMA7D65 Curiosity Board**

## 6.1 Conversion of PyTorch to ONNX Model
"""

from ultralytics import YOLO

# Load the YOLO11 model
model = YOLO("/content/gdrive/MyDrive/yolov11n_project/train/weights/best.pt")

# Export the model to ONNX format
model.export(format="onnx")  # creates 'yolo11n.onnx'

"""Required packages for onnx2tf conversion"""

!pip install onnx2tf
!pip install onnx-graphsurgeon
!pip install ai_edge_litert
!pip install sng4onnx

!onnx2tf -i /content/gdrive/MyDrive/yolov11n_project/train/weights/best.onnx --b 1

"""## 6.2 Quantising the Model For Edge Deployment

Randomly sample 200 images for calibration.
Store in a new directory
"""

import os
import random
import shutil

# Paths
source_dir = "/content/data/train/images"
calib_dir = "/content/calibration_images"
num_images = 200  # Change this number depending on your calibration needs

# Create target folder
os.makedirs(calib_dir, exist_ok=True)

# Get all image filenames
image_files = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
selected_files = random.sample(image_files, min(num_images, len(image_files)))

# Copy selected files
for file in selected_files:
    shutil.copy(os.path.join(source_dir, file), os.path.join(calib_dir, file))

print(f"‚úÖ Copied {len(selected_files)} images to {calib_dir}")

"""This function defines the representative dataset generator required for post-training quantization of the YOLO model. It loads calibration images, resizes and normalizes them, and yields them in the correct format for use during TFLite model conversion.

"""

import cv2
import numpy as np
import os

calib_dir = "/content/calibration_images"
calib_images = sorted(os.listdir(calib_dir))

def representative_dataset_gen():
    for img_name in calib_images:  # List of calibration images
        img_path = os.path.join(calib_dir, img_name)
        img = cv2.imread(image_path)
        img = cv2.resize(img, (320, 320))
        img = img.astype(np.float32) / 255.0  # Normalize to [0,1] if needed
        img = np.expand_dims(img, axis=0)  # Shape (1,320,320,3)
        yield [img]

"""The following code converts a TensorFlow SavedModel to a fully integer-quantized TensorFlow Lite model using a representative dataset for calibration. This optimization reduces model size and improves inference efficiency on edge devices that support INT8 operations.








"""

import tensorflow as tf
# Load your TensorFlow model
model = tf.saved_model.load('/content/saved_model')  # Assuming you exported to SavedModel first

# Create TFLite Converter
converter = tf.lite.TFLiteConverter.from_saved_model('/content/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen

# Set fully integer quantization
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

# Important: ensure input/output are int8
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

# Convert
tflite_quant_model = converter.convert()

# Save the TFLite model
with open('yolov11n_int8.tflite', 'wb') as f:
    f.write(tflite_quant_model)

"""## 6.3 Running Inference on the TFLite Model

This code loads a TensorFlow Lite model and performs inference on all images in the prediction folder. It preprocesses each image by resizing and normalizing it to match the model‚Äôs expected input shape, runs the inference, and prints the output tensor shape. The resized images are saved separately for visualization or further processing.


"""

import os
import cv2
import numpy as np
import tensorflow as tf

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/saved_model/best_float16.tflite")
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']  # e.g., [1, 320, 320, 3]
print("Model expects input shape:", input_shape)

# Paths
image_dir = "/content/runs/detect/predict/"
resized_dir = "/content/resized_images"
os.makedirs(resized_dir, exist_ok=True)

# Supported formats
image_extensions = (".jpg", ".jpeg", ".png")

# Loop over images
for filename in os.listdir(image_dir):
    if filename.lower().endswith(image_extensions):
        input_path = os.path.join(image_dir, filename)
        output_path = os.path.join(resized_dir, filename)

        # Load and preprocess image
        img = cv2.imread(input_path)
        img_resized = cv2.resize(img, (input_shape[2], input_shape[1]))  # Width x Height
        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
        img_norm = img_rgb.astype(np.float32) / 255.0
        input_tensor = np.expand_dims(img_norm, axis=0)

        # Run inference
        interpreter.set_tensor(input_details[0]['index'], input_tensor)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])

        print(f"[{filename}] Output shape:", output_data.shape)
        # Optionally: decode predictions here (e.g., NMS, thresholding)

        # Save resized image
        cv2.imwrite(output_path, img_resized)
                # Postprocess YOLO-style output
        def sigmoid(x):
            return 1 / (1 + np.exp(-x))

        pred = np.squeeze(output_data)  # shape: (40, 8400)
        pred = np.transpose(pred, (1, 0))  # shape: (8400, 40)

        boxes = []
        scores = []
        class_ids = []

        for det in pred:
            x, y, w, h = det[:4]
            conf = sigmoid(det[4])
            class_confs = sigmoid(det[5:])
            class_id = np.argmax(class_confs)
            score = conf * class_confs[class_id]

            if score > 0.3:  # Confidence threshold
                # Convert from center x,y,w,h to x1,y1,x2,y2
                x1 = int((x - w / 2) * img.shape[1])
                y1 = int((y - h / 2) * img.shape[0])
                x2 = int((x + w / 2) * img.shape[1])
                y2 = int((y + h / 2) * img.shape[0])
                boxes.append([x1, y1, x2 - x1, y2 - y1])
                scores.append(float(score))
                class_ids.append(class_id)

        # Apply NMS
        indices = cv2.dnn.NMSBoxes(boxes, scores, score_threshold=0.3, nms_threshold=0.5)

        for i in indices:
            i = i[0] if isinstance(i, (list, np.ndarray)) else i
            box = boxes[i]
            x, y, w, h = box
            cv2.rectangle(img_resized, (x, y), (x + w, y + h), (0, 255, 0), 2)
            label = f"{class_ids[i]}: {scores[i]:.2f}"
            cv2.putText(img_resized, label, (x, y - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        # Save visualization
        vis_path = os.path.join(resized_dir, f"vis_{filename}")
        cv2.imwrite(vis_path, img_resized)

from IPython.display import display
from PIL import Image

# Display first 5 visualized outputs
for fname in sorted(os.listdir(resized_dir)):
    if fname.startswith("vis_"):
        img_path = os.path.join(resized_dir, fname)
        display(Image.open(img_path))